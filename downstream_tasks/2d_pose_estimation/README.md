# 2D Pose Estimation

## ğŸ“š Contents
- [2D Pose Estimation](#2d-pose-estimation)
  - [ğŸ“š Contents](#-contents)
  - [ğŸ“‹ Introduction](#-introduction)
  - [ğŸ“‚ Datasets](#-datasets)
  - [ğŸ› ï¸ Environment](#ï¸-environment)
  - [ğŸš€ Get Started](#-get-started)
  - [ğŸ’— Acknowledgement](#-acknowledgement)
  - [ğŸ¤ Contribute \& Contact](#-contribute--contact)

## ğŸ“‹ Introduction

The HAP pre-trained model is fine-tuned for the 2d human pose estimation task with respect to:

- Three datasets: MPII, COCO, and AIC (AI Challenger)
- Two resolution sizes: (256, 192) and (384, 288)
- Two training settings: single-dataset and multi-dataset training

## ğŸ“‚ Datasets

Put the dataset directories outside the HAP project:
```bash
home
â”œâ”€â”€ HAP
â”œâ”€â”€ mpii  # MPII dataset directory
â”‚   â”œâ”€â”€ annotations
â”‚   â”‚   â”œâ”€â”€ mpii_train.json
â”‚   â”‚   â””â”€â”€ mpii_val.json
â”‚   â””â”€â”€ images
â”‚       â”œâ”€â”€ xxx.jpg
â”‚       â””â”€â”€ ...
â”œâ”€â”€ coco  # COCO dataset directory
â”‚   â”œâ”€â”€ annotations
â”‚   â”‚   â”œâ”€â”€ person_keypoints_train2017.json
â”‚   â”‚   â””â”€â”€ person_keypoints_val2017.json
â”‚   â”œâ”€â”€ train2017
â”‚   â”‚   â”œâ”€â”€ xxx.jpg
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ val2017
â”‚       â”œâ”€â”€ xxx.jpg
â”‚       â””â”€â”€ ...
â””â”€â”€ aic  # AIC dataset directory
    â”œâ”€â”€ annotations
    â”‚   â”œâ”€â”€ aic_train.json
    â”‚   â””â”€â”€ aic_val.json
    â”œâ”€â”€ ai_challenger_keypoint_train_20170902
    â”‚   â””â”€â”€keypoint_train_images_20170902
    â”‚       â”œâ”€â”€ xxx.jpg
    â”‚       â””â”€â”€ ...
    â””â”€â”€ ai_challenger_keypoint_validation_20170911
        â””â”€â”€keypoint_validation_images_20170911
            â”œâ”€â”€ xxx.jpg
            â””â”€â”€ ...
```

## ğŸ› ï¸ Environment
Conda is recommended for configuring the environment:
```bash
conda env create -f env-2d_pose.yaml && conda activate env_2d_pose

# Install mmcv and mmpose of ViTPose
cd mmcv && git checkout v1.3.9 && MMCV_WITH_OPS=1 && cd .. && python -m pip install -e mmcv
python -m pip install -v -e ViTPose
```

## ğŸš€ Get Started

It may need 8 GPUs with memory larger than 16GB, such as NVIDIA V100, for single-dataset training with resolution of (256, 128).

It may need 8 GPUs with memory larger than 40GB, such as NVIDIA A100, for single-dataset training with resolution of (384, 288) and multi-dataset training.

```bash
# -------------------- Fine-Tuning HAP for 2D Pose Estimation --------------------
cd HAP/downstream/2d_pose_estimation/
ViTPose/

# Download the checkpoint
CKPT=ckpt_default_pretrain_pose_mae_vit_base_patch16_LUPersonPose_399.pth  # checkpoint path

rm -rf mmcv_custom/checkpoint.py
cp mmcv_custom/checkpoint-hap.py mmcv_custom/checkpoint.py

# ---------- For Single-Dataset Training ----------
DATA=mpii  # {mpii, coco, aic}
RESOLUTION=256x192  # {256x192, 384x288}
OUTPUT_DIR=output-2d_pose_estimation/${DATA}/${RESOLUTION}/
CONFIG=configs/body/2d_kpt_sview_rgb_img/topdown_heatmap/${DATA}/ViTPose_base_${DATA}_${RESOLUTION}.py

# ---------- For Multi-Dataset Training ----------
# RESOLUTION=256x192  # {256x192, 384x288}
# CONFIG=configs/body/2d_kpt_sview_rgb_img/topdown_heatmap/coco/ViTPose_base_coco+aic+mpii_${RESOLUTION}.py
# OUTPUT_DIR=output-person_reid/coco+aic+mpii/${RESOLUTION}/

python -m torch.distributed.launch \
   --nnodes ${NNODES} \
   --node_rank ${RANK} \
   --nproc_per_node ${NPROC_PER_NODE} \
   --master_addr ${ADDRESS} \
   --master_port ${PORT} \
   tools/train.py \
   ${CONFIG} \
   --work-dir ${OUTPUT_DIR} \
   --launcher pytorch \
   --cfg-options model.pretrained=${CKPT}
   # --resume-from
```

After multi-dataset training, split the model and evaluate it on MPII and AIC (COCO has been tested during training) : 

```bash
# -------------------- Split Model --------------------
# We simply split the latest one. Maybe you can choose the best one.
python tools/model_split.py \
   --source ${OUTPUT_DIR}latest.pth

# -------------------- Test on MPII and AIC --------------------
DATA=mpii  # {mpii, aic}
TEST_MODEL=${DATA}.pth
RESOLUTION=256x192  # {256x192, 384x288}
CONFIG=configs/body/2d_kpt_sview_rgb_img/topdown_heatmap/${DATA}/ViTPose_base_${DATA}_${RESOLUTION}.py

python -m torch.distributed.launch \
   --nproc_per_node=${NNODES} \
   --node_rank ${RANK} \
   --nproc_per_node ${NPROC_PER_NODE} \
   --master_addr ${ADDRESS} \
   --master_port ${PORT} \
   tools/test.py \
   ${CONFIG} \
   ${OUTPUT_DIR}${TEST_MODEL} \
   --launcher pytorch
``` 

## ğŸ’— Acknowledgement

Our implementation is based on the codebase of [ViTPose](https://github.com/ViTAE-Transformer/ViTPose), [mmcv](https://github.com/open-mmlab/mmcv), [mmpose](https://github.com/open-mmlab/mmpose).

## ğŸ¤ Contribute & Contact

Feel free to star and contribute to our repository. 

If you have any questions or advice, contact us through GitHub issues or email (yuanjk0921@outlook.com).